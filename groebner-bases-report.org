#+title: Groebner Bases
:PREAMBLE:
#+OPTIONS: todo:nil H:9 tags:t tex:t toc:nil
#+STARTUP: overview
#+AUTHOR: Ryan Greenup
#+PLOT: title:"Citas" ind:1 deps:(3) type:2d with:histograms set:"yrange [0:]"
#+TODO: TODO IN-PROGRESS WAITING DONE
#+CATEGORY: Note
:END:
:HTML:
#+INFOJS_OPT: view:info toc:3
#+HTML_HEAD_EXTRA: <link rel="stylesheet" type="text/css" href="~/Templates/CSS/Org-CSS/bigblow.css">
# #+CSL_STYLE: /home/ryan/Templates/CSL/nature.csl
:END:
:R:
#+PROPERTY: header-args:R :session TADMain :dir ./ :cache yes :eval never-export :exports both
#+PROPERTY: :eval never 
# exports: both (or code or whatever)
# results: table (or output or whatever)
:END:
:LATEX:
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [a4paper,11pt,twoside]
#+LATEX_HEADER: \IfFileExists{/home/ryan/Templates/Org_Mode_Report/resources/style.sty}{\usepackage{$HOME/Templates/Org_Mode_Report/resources/style}}{}
#+LATEX_HEADER: \IfFileExists{$HOME/Templates/Org_Mode_Report/resources/referencing.sty}{\usepackage{$HOME/Templates/Org_Mode_Report/resources/referencing}}{}
# AddBibResource without if test using =~= char because org-ref is finicky
#+LATEX_HEADER: \addbibresource{./ref.bib}
#+LATEX_HEADER: \usepackage[mode=buildnew]{standalone}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usetikzlibrary{decorations.fractals}
#+LATEX_HEADER: \usetikzlibrary{lindenmayersystems}
:END:

Groebner bases is one of the main practical tools for solving systems
of polynomial equations.

* TODO COMMENT Exemplar
** Single Variable Non Linear
*** Linear
    Observe that it's the solution

*** Non Linear
    Observe that it's the GCD

** Two Variable
*** Linear
*** Non Linear
** Many Variables Non LInear
   Just Visualize a few of these to get the point across.

* Summary
  Much of the theory of Groebner Basis is buried under needless
  amounts of abstract algebra, this is for the most part unnecessary
  and if I were to begin this investigation again I would first
  implement Buchberger's Algorithm, manually, using /Sympy/ by referring to:

    - Lectures 14 and 15 of Andreas Schulz OCW Integer Programming
      Course cite:andreasschulzIntegerProgrammingCombinatorial
    - Chapters 1-2 of /Ideals, Varieties and Algorithms/ cite:coxIdealsVarietiesAlgorithms1997
    - Lecture 15 of Judy Holdenner's course on Algebraic Geometry cite:judyholdenerAlgebraicGeometry2013
    - Lecture 14 of Pablo Parrilo's course on Algebraic Techniques cite:pabloparriloAlgebraicTechniquesSemidefinite
    - The /Sympy/ source code for:
      - =polys.roebnertools= cite:sympydevelopmentteamSympyPolysGroebnertools
      - =solvers.polysys= cite:sympydevelopmentteamSympySolversPolysys
    - The /Sympy/ documentation for /Polynomial Manipulation/ cite:sympydevelopmentteamGrobnerBasesTheir2021

  Unfortunately this was not an option for me as these resources were
  not known to me until very late in the investigation. I hope that
  this report can serve as a guide for others who pick up this topic
  such that they can:

    - Come to grips with the core concepts and practical applications
      quickly without wasting time on abstract algebra that is poorly
      explained [fn:2]
    - Identify useful resources that are written well and written with
      accessibility in mind
    - Avoid material that serves as, for lack of a better word, as a
      red herring.
   
  Although =sympy= is probably not the best tool for studying
  commutative algebra specifically (and the implementation may not be
  battle tested either, see e.g. cite:WrongGroebnerBasis), the simple
  and accessible nature of =sympy= made it's documentation by far the
  most valuable resource for grappling with this topic.

  An extension to this investigation would be to:

    - Try and implement Buchberger's Algorithm from scratch using
      functions and iterations in /Python/ in order to return a Reduced
      Groebner Basis
    - Try to demonstrate, in good detail, the relationship between
      the Euclidean Algorithm and Buchberger's Algorithm
    - Try to implement Buchberger's Algorithm using /Normal Selection
      Strategy/ [[cite:hibiGrobnerBasesStatistics2014][\S 3.1.2]], see also cite:sympydevelopmentteamSympyPolysGroebnertools,prof.berndsturmfelsIntroductionGrobnerBases2017.

** Further Resources
   The following resources may be useful as reference material, but I
   would advice against using these as any sort of primary material,
   in order of recommendation (but not necessarily relevance)



     1. Judson, T. W., & Open Textbook Library, Abstract algebra
        theory and applications cite:judsonAbstractAlgebraTheory2016
     2. Howlett, R., An undergraduate course in Abstract Algebra:
        Course notes for MATH3002 cite:roberthowlettUndergraduateCourseAbstract
     3. Lee, G., Abstract Algebra cite:gregoryleeAbstractAlgebra2018
     4. Grillet, P. A., Abstract Algebra cite:grilletAbstractAlgebra2007
     5. Hibi, T., Grobner Bases: Statistics and Software Systems cite:hibiGrobnerBasesStatistics2014
     6. Adams, W. W., & Loustaunau, P., An introduction to Gröbner bases cite:adamsIntroductionGrobnerBases1994
     7. Nicodemi, O., Sutherland, M. A., & Towsley, G. W., An
        introduction to abstract algebra with notes to the future
        teacher cite:nicodemiIntroductionAbstractAlgebra2007a

  
   Further material that I haven't had a chance to look throughA:
   includes:

   - Becker, T., Weispfenning, V., & Kredel, H., Gröbner bases: a computational approach to commutative algebra cite:beckerGrobnerBasesComputational1993

* Introduction
  A Groebner Basis is a set of polynomials that spans the solution
  space of another set of polynomials, they are of interest to us
  because they are useful for solving systems of polynomial equations
  and provide a generalized theory that shows the relationships between: 

    - Polynomial Long Division with multiple variables and divisors,
      see e.g. [[cite:coxIdealsVarietiesAlgorithms1997][\S 3]]
    - The Division Algorithm see
      e.g. [[cite:coxIdealsVarietiesAlgorithms1997][\S 3]] and cite:nicodemiIntroductionAbstractAlgebra2007a
    - The LCM and GCD [[cite:coxIdealsVarietiesAlgorithms1997][\S 2.6]]
    - The Euclidean Algorithm and Gaussian Elimination
      - Both of which provide output that are special cases of
        Groebner Bases.

  The theory of Groebner Bases even provides a framework to
  re-express the Fundamental Theorem of Algebra cite:sturmfelsSolvingSystemsPolynomial2002.
* Polynomials
  Let \(K\) be some field (typically \(\mathbb{Q}, \mathbb{R},
\mathbb{C}\), see \S [[#algebra-fields]] for more information)

** Monomials
   A /monomial/ in the variables \(x_1, x_2, \ldots x_n\) is given by: [[cite:hibiGrobnerBasesStatistics2014][p. 3]]

    \[ 
   \begin{aligned}
   \prod^n_{i=1} \left[ x_i^{a_i} \right] = x_1^{1_1} \cdot x_2^{a_2} \cdot
   x_3^{a_3} \ldots x_n^{a_n} \quad : a \in \mathbb{Z^+}
   \end{aligned}\]

   Note however that \(a\) must be a non-negative integer [[cite:e.h.connellElementsAbstractLinear2001][p. 48]]
*** Degree
    The degree is given by the sum of the exponents, so:
   
    $$
    \mathrm{deg}\left(   \prod^n_{i=1} \left[ x_i^{a_i} \right]   \right) =
    \sum^{n}_{i= 1}   \left[ a_i \right] 
    $$

*** Terms
    A term is a monomial with a non-zero coefficient, so for example:

    $$
    17 \cdot x_1^3 \cdot x_2^5 \cdot x_3^{13}
    $$

    Is a term with degree 21 $(3+5+13)$ and a coefficient of 17.
*** Polynomials
    A polynomial is a finite sum of terms, the degree of which
    is defined to be the maximum degree of any of the terms.
   
**** Exception
     The polynomial:

     $$
     f = 0
     $$

     Has an undefined degree. Terms only have a *non-zero* coefficient,
     hence \(0\) doesn't have any terms and so the definition of degree
     doesn't work for it.
    
     Whereas \(f=c, \quad \exists c \in \mathbb{C}\) does have 1 term,
     for which the degree is 0.

**** Support of a polynomial
     The support of a polynomial \(f\) is the set of monomials
     appearing in \(f\), e.g. for the following 6th degree polynomial
     in 2 variables, the support of that polynomial is given by:

     \[
     f(x) = x^2+3x^3+4y \implies \mathrm{supp}\left(f\right) =
     \left\{x^2, 3x^3, 4y\right\}
     \]

     The initial of the support \(\mathrm{in}_{\prec}\left(f\right)\)
     is the polynomial with the highest ranking with respect to some
     ordering of the monomials (see \S [[monomial-orders]]) [[cite:hibiGrobnerBasesStatistics2014][1.1.5]].

      
      
***** Other Terminology

      The following terms are commonly used: [[cite:coxIdealsVarietiesAlgorithms1997][\S 2.2]]

        - The \(\mathrm{multidegree}\left(f\right)\), is the
          largest power of any variable of any monomial in a polynomial
        - The Leading coefficient \(\mathrm{LC}\left(f\right)\) is the
          term corresponding to the monomial containing the variable
          that corresponds to the multidegree
        - The Leading monomial \(\mathrm{LM}\left(f\right)\) is the
          monomial corresponding containing the variable
          that corresponds to the multidegree
        - The Leading term \(\mathrm{LT}\left(f\right)\) is the product
          of the leading coefficient and the leading monomial

	So for example, in the polynomial:

	\[
	f= 4x^2y^2 + 3x^3 + 7xy
	\]

	  - The initial is \(4x^2y^2\)
	  - The Leading Coefficient is 3
	  - The Leading Monomial is \(x^3\)
	  - The Leading Term [fn:3] is \(x^3\)

	 
	 

**** Homogenous Polynomial
     If all terms of a polynomial have an equal degree (say \(\exists q
     \in \mathbb{N}\) Then that polynomial is said to be a /homogenous
     polynomiial of degree \(q\)/, e.g.

       $$
       x_1^{3}\cdot x_2^{4} \cdot x_3^{2} + x_1^{6}\cdot x_5^{2} \cdot x_7
     $$
    
     is a homogenous polynomial of degree 7.

**** The Polynomial Ring

     The Rings, Vectors and Polynomials

     Let \(K\left[x_1, x_2, x_3, \ldots x_n\right]=K\left[\mathbf{X}_n\right]\) denote the set of
     all polynomials in the variables \(x_1, x_2, x_3, \ldots x_n\)
     with coefficients in some field \(K\).

     If \(f\) and \(g\) are polynomials from \(K\left[x_1, x_2, x_3,
     \ldots x_n\right]\) with addition and multiplication defined in
     the ordinary way (i.e. just normal algebra), then \(K\left[x_1,
     x_2, x_3, \ldots x_n\right]\) forms an algebraic structure known
     as a Ring.

     Readers  may be familiar with the axioms of a vector space, for
     which the set of polynomials in  \(K\left[x_1\right]\) of degree
     less than \(n\) also satisfies [[cite:larsonElementaryLinearAlgebra1991a][\S 4.4]], a ring structure is much
     the same concept, it's a set with specific characteristics.
     One of the main differences is that while a vector
     space requires a scalar multiplicative identity, a ring
     structure does not.

     On the other hand not all vector spaces are necessarily rings
     because they are not necessarily closed under multiplication
     (although defining multiplication by element-wise product would
     remedy this), see \S [[#algebra-rings]] for more information.

* Ideals and Varieties
** Affine Space 
   The affine \(n\)-space of some field \(K\) is given by: [[cite:coxIdealsVarietiesAlgorithms1997][\S 1.1]]

   \[
   K^n=\left\{\left(a_1, a_2, a_3, \ldots, \a_n\right) \mid a_i \in K, \forall i \in \mathbb{Z}^+\right\}
   \]

   For example if \(K\) was given by \(\mathbb{R}\) the resulting
   affine \(n\)-space would be the /Cartesian Plane/.

** Zero Point
   The zero-point of some function \(f\in K\left[\mathbf{X}_n\right]
   \) is a point in $K^n$:  cite:hibiGrobnerBasesStatistics2014

   \begin{align*}
         f\left( a_1, a_2, a_3 \ldots a_n \right) =0
   .\end{align*}

   In the broader context of equations rather than specifically
   functions, these points are often referred to as roots.

   These points are often referred to as roots
   [[cite:judsonAbstractAlgebraTheory2016][\S 17.2]], however this is
   usually in the context of equations more broadly rather than
   functions specifically. cite:82645
** Variety
   Consider a set of functions \(F=\left\{ f_{1},f_{2},f_{3},\ldots
   f_{s}\right\}\), the variety of this set of functions is denoted
   \(\mathbf{V}\left(F\right)\) and is the set of all zero-points of
   all the functions:

   \[
   \boldsymbol{V}\left(F\right)=\left\{ \left(a_{1},a_{2},a_{3},\ldots a_{n}\right)\in K^{n}\mid f_{i}\left(a_{1},a_{2},a_{3},\ldots a_{n}\right),\forall i\in\mathbb{Z}^{+}<s\right\} 
   \]

   The convention is that all functions in \(F\) are set to be equal
   to 0, and if this convention is taken, the variety of that set is
   the set of solutions corresponding to that set of equations.
*** Example
    Consider for example the set \(\left\{ -y+x^{2}-1,-y+1\right\}\),
    the solution to this system can be found by substitution:


    \begin{align*}
       -y + x^{2}-1	&=0=-y+1 \\
       x^{2}-1	&=y=1 \\
       x^{2}	&=2   \\
       x	&=\pm\sqrt{2}
   \end{align*}

   and so:

   \[
   \boldsymbol{V}\left(\left\{ -y+x^{2}-1,-y+1\right\}\right)=\left\{ \left(-\sqrt{2},1\right),\left(\sqrt{2},1\right)\right\} 
   \]

** Ideals
   Ideals are a set with a particularly convenient property, given
   functions \(f,g\in K\left[\mathbf{X}_n\right]\), a subring
   $I\subset K\left[\textbf{X}\right]$ is said to be an ideal if it
   is closed under addition and admits other functions under
   multiplication: [[cite:hibiGrobnerBasesStatistics2014][\S 1.1.3]]
 
   1. $f\in I \land g \in I \implies f+g \in I$
   2. $f\in I \land g \in k\left[ \textbf{X} \right] \implies gf \in I$
 
   So for example, $\left\{0\right\}$ is an ideal of the polynomial ring in
   all variables, and as a matter of fact $0\in I$ for all ideals of
   polynomial rings in all variables.

   A subring is a subset that is itself a ring, so \(I\) would be a
   subset that is closed under addition and multiplication and
   contains an additive identity (i.e. \(0 \in I\)). [fn:1] As a
   matter of fact it can be shown that:

     + \(0\in I\) 
     + \(\left\{0\right\}\) is an ideal 

   for all ideals in all variables and that is an ideal (because
   otherwise the result would not be admitted to \(I\)).
*** Example

    Let \(R = \mathbb{Z}\) and \(I=2\mathbb{Z}\), the set of
    \(\mathbb{Z}\) is a commutative ring with unity, \(2\mathbb{Z}
    \subset \mathbb{Z}\) is:

      1. \(2\mathbb{Z} \neq \emptyset\)
      2. closed under multiplication and addition
      3. admits any other integer under multiplication (i.e. even
         \(\times\) anything is even)
** Ideals and Varieties
   If we have a variety of \(V \subset K^n \), we denote, $I\left( V \right)$ as the set of all
  polynomials $f_i\in k\left[ \textbf{X} \right]$ : [[cite:hibiGrobnerBasesStatistics2014][\S 1.1.3]]

  \begin{align*}
        f_i\left( a_1, a_2, a_3, \ldots a_n \right) =0, \quad \forall \left( a_1, a_2, a_3, \ldots a_n \right) \in V
  .\end{align*}


  this set of functions satisfies the properties of an ideal and is known as the ideal of V cite:coxIdealsVarietiesAlgorithms1997.

  In other words, the ideal of the variety of a set of functions,
  $I\left( \mathbf{V}\left(F\right) \right)$, is the set of,
  polynomials, that have the same zero-points as the simultaneous zero
  points of all functions in \(F\).

** Generating Ideals
   The ideal generated by \(F\) is:

   \[
    \left\langle F\right\rangle =\left\{
    p_{1}f_{1}+p_{2}f_{2}+p_{3}f_{3}+\ldots p_{n}f_{n}\mid f_{i}\in
    F,p_{i}\in K\left[\boldsymbol{X}\right],\forall
    i\in\mathbb{Z}^{+}\right\}
    \]

    Such a set satisfies the properties of an ideal and is a subset
    of the functions that share the zero-points with \(F\): [[cite:coxIdealsVarietiesAlgorithms1997][p. 34]]

    \[
    \left\langle F \right\rangle \subseteq
    I\left(\boldsymbol{V}\left(F\right)\right)
    \]


    \(\left\langle F \right\rangle\) is the set of all the linear combinations of elements in \(F\)
    with polynomials in \(K\left[\mathbf{X}_n\right]\), another way
    to phrase it would be that \(\left\langle F\right\rangle\) is the
    set of polynomial consequences of \(F\) [[cite:coxIdealsVarietiesAlgorithms1997][p. 30]].

    If some *finite* set of polynomials \(F\), can generate an ideal
    \(I\), it is said that \(I\) is finitely generated and that \(F\)
    is a basis for \(I\). Every ideal in
    \(K\left[\mathbf{X}_n\right]\) is finitely generated
    [[cite:coxIdealsVarietiesAlgorithms1997][p. 77]], this is known
    as /Hilbert's Basis Theorem/, this is important because it means we
    if we had an algorithm that involved taking different polynomials
    from such a basis, that algorithm would eventually end.

    If two sets are bases of the same ideal, they will have the same
    variety, i.e. if two sets can generate the same set of functions,
    they'll have the same solutions (assuming that the set of
    functions is an ideal), this also implies
     

*** Initial Ideal
  The initial ideal:

  \[\left\langle
  \mathrm{in}_{\prec}\left(I\right)\right\rangle =\left\langle
  \left\{ \mathrm{in}_{\prec}\left(f\right):0\neq f\in I\right\}
  \right\rangle \]

  is generated by infinitely many monomials, namely
  the initial monomials, for the infinitely many polynomials in the
  ideal I. [[cite:hibiGrobnerBasesStatistics2014][\S 1.1.5]]

  It's common also to see a similar definition for the ideal
  generated by the leading terms is denoted \(\left\langle
  \mathrm{LT}\left(f\right)\right\rangle \) [[cite:coxIdealsVarietiesAlgorithms1997][\S 2.5]].

*** Comparison with Linear Algebra
    If \(S\) is some set of vectors and every vector in a vector
    subspace \(V\) can be written as a linear combination of the
    elements of \(S\) is is said that \(S\) spans \(V\), so for
    example \(S=\left\{ \left\langle 1,0\right\rangle ,\left\langle
    0,1\right\rangle \right\}\) spans \(\mathbb{R}^2\) or
    \(S=\left\{1, x, x^2\right\}\) spans \(P_2\).

    Ideals for rings are similar in nature to vector subspaces and
    normal subgroups. It's worth drawing attention to the fact that
    that the term basis in the context of an ideal (which could be
    more accurately called a generating set
    cite:sturmfelsSolvingSystemsPolynomial2002) is quite different
    from a linear basis [[cite:coxIdealsVarietiesAlgorithms1997][p. 35]].

    In linear algebra a basis spans and is linearly independent, the
    basis of an ideal however only spans, there is no independence,
    for example:
  
      $$\begin{aligned}
  f_{1}\left(x,y\right)=y\quad  \quad & \vec{v}_{1}=\left\langle 0,1\right\rangle \\
  f_{2}\left(x,y\right)=x \quad \quad & \vec{v}_{2}=\left\langle
  1,0\right\rangle \end{aligned}$$    
  
    Linear independence is generally satisfied if linear
    combination is equal to zero, only if the multiplying terms are
    zero, i.e. \(f_1\) and \(f_2\) are linearly independent only if:

  
      $$\begin{aligned}
  0 & =a\left\langle 0,1\right\rangle +b\left\langle 1,0\right\rangle ,\quad\forall a,b\in\mathbb{R}\\
  & =\left\langle a,b\right\rangle \\
  & \implies a=b=0\end{aligned}$$    
  

    This clearly doesn't work for polynomials, however, because setting $g_{i}=x$ and
    $g_{j}=-y$ satisfies such an equation.
  
    $$0=g_{i}y+g_{j}x,\quad\not\!\!\!\implies g_i=g_j=0, \quad \forall g_{i}g_{j}\in
    k\left[\mathbf{X}\right] $$
  

  So linear independence doesn't have a lot of meaning with polynomials, 
  it's only the spanning property that is meaningful.
*** COMMENT Example


    In two or more variables we can't know how many polynomials
    generate the ring, so there is no Euclidean algorithm, we only
    know that it is finitely generated.

    For example:

    $$\begin{aligned}
    \left\langle \left\{ 2x+3y+4z-5,\enspace3x+4y+5z-2\right\} \right\rangle \\
    =\\
    \left\langle \left\{ x-z+14,\enspace y+z-11\right\} \right\rangle \end{aligned}$$

    but the second set reveaals more information
* Initials and Leading Monomials
** Monomial Ordering
   Monomials are ordered by degree, e.g. \(x \prec x^2\) or \(xyz
   \prec x^2yz\), however in many variables it isn't always clear
   which order should be chosen, for example the following monomials
   have the same degree and if they are ordered by the value on first variable:

   \[
   xy^3 \prec x^2yz 
   \]

   If however they are ordered by trying to minimize the last
   variable:

   \[
   x^2yz \prec xy^3
   \]

   Recall from polynomial long division that the first term in a
   polynomial important to the algorithm, for a similar reason it is
   necessary to decide before hand on an ordering, and generally in
   this report the lexicographic order (i.e. alphabetical) will be
   used.

   This isn't as important as many texts make it out to be and so
   further discussion appears further below in \S [[#monomial-orders]].
* Groebner Bases
    A finite subset \(G\) of an ideal \(I\) is a Grobner Basis, (with
    respect to some term order \(\prec\), if: cite:berndsturmfelsIntroductionGrobnerBases2017a,hibiGrobnerBasesStatistics2014

    \[
    \left\{ \mathrm{in}_{\prec}\left(g\right)\mid g\in G\right\} 
    \]

    generates \(\left\{ \mathrm{in}_{\prec}\left(I\right)\right\}\)

    It's common also to see this definition reformulated with respect
    to leading terms as opposed to initial monomials, in which case
    \(G\) is said to be a Groebner Bases if: [[cite:coxIdealsVarietiesAlgorithms1997][2.5]]

    \[
    \mathrm{LT}\left(I\right)=\left\langle \mathrm{LT}\left(g_{1}\right),\mathrm{LT}\left(g_{2}\right),\mathrm{LT}\left(g_{3}\right),\ldots\mathrm{LT}\left(g_{n}\right)\right\rangle 
    \]

    there are many such generating sets, we can add any element to G
    to get another Groebner Basis, so in practice we may be more
    concerned with reduced Groebner Basis. Note also that even though
    the leading term is different from the initial monomial, either
    can be used to define a Groebner Bases, however it is not yet
    clear to me if the Groebner Bases will depend on the monomial
    ordering \(\prec\) only if the initial is used to define it.

    The variety of a set of functions depends only on the ideal of
    \(F\), if two sets generate the same ideal they have the same
    variety and if \(G\) is
    a Grobner Basis for F, then \(V(G)=V(F)\). 

    The reason we care about a Groebner Bases more generally is
    because the set tends to provide more information of the solution space.

* Buchberger's Criterion
    \(G\) is a Groebner basis, if and only if, every \(S\)-polynomial
    formed by any two pairs from \(G\) has a remainder of 0, where
    the S-polynomial is given by: [[cite:coxIdealsVarietiesAlgorithms1997][\S 2.6]]

    \[
    S\left(f,g\right)=\mathrm{lcm}\left(\mathrm{LM}\left(f\right),\mathrm{LM}\left(g\right)\right)\times\left(\frac{f}{\mathrm{LT}\left(f\right)}-\frac{g}{\mathrm{LT}\left(g\right)}\right)
    \]
    The remainder that we are concerned with is:

    \[
    r = {\overline{S(f,g)}^{_G}} = S(f,g) \mod \prod_{g\in G} \left(G \right)
    \]
* Bucherger's Algorithm
  Buchberger's Algorithm takes a set of polynomials, \(F\) and
  eventually returns another set \(G\) which is a Groebner Bases.

    To do this the algorithm tests every pair of polynomials in F with
    the criterion above, if the remainder for any pair is non zero,
    it is placed into \(F\) as another polynomial. 
    Once every combination has been considered, the original set
    \(F\) will be a Groebner Basis.

** Reduced Groebner Basis
   A reduced Groebner Basis is a Groebner Basis that has needless
   polynomials discarded, I have not had time to investigate these
   yet.

** Examples
   for examples of Buchberger's Algorithm, refer to the attached
   /Jupyter Notebook/, this is quite sparse as resources to understand
   the algorithm were discovered quite late in the investigation as
   was the realisation that use =sympy= had a significant amount of
   documentation on the algorithm.
* Abstract Algebra
  The following are concepts that are /nice to have/ in understanding
  the topic, but are not strictly necessary to get a broad
  understanding of the topic.

  They were needlessly investigated early on because accessible
  resources
  (e.g. cite:coxIdealsVarietiesAlgorithms1997,andreasschulzIntegerProgrammingCombinatorial,sympydevelopmentteamSympyPolysGroebnertools)
  had not yet been discovered.
** Background
*** Algebra
**** Relations
     A relation on a set \(A\) is a subset \(R\) of the Cartesian
     product:

     \[
     A\times A=\left\{ \left(a,b\right):\enspace a,b\in A\right\}
     \]

     If \((a,b)\in R\) it is written that \(a\enspace R \enspace b\).
***** Example
      :PROPERTIES:
      :CUSTOM_ID: relation-example
      :END:
      The example most relevant to the theory of Groebner bases [fn:33] is
      the ~<~ relation. If we had the set \(A = \left\{ 1,\ 2,\ 3 \right\}\)
  
     The cartesian product would be:

     \begin{align*}
   A\times A=\Bigg\{	&\left(1,1\right),\left(1,\ 2\right),\left(1,3\right), \\
			   &\left(2,1\right),\left(2,2\right),\left(2,3\right), \\
			   &\left(3,\
   1\right),\left(3,2\right),\left(3,3\right)\quad\Bigg\}
   \end{align*}

     The set corresponding to the relation < would be:
  
     \(\left\{ \left( 1,2 \right),\ \left( 1,3 \right),\ \left( 2,3 \right) \right\}\)

     and so it is said that:

       - \(1<2\)
       - \(1<3\)
       - \(2<3\)
      
***** Types of Relations

     + *Reflexive* relations are relations where
       * \(\ \forall\ a \in A, a\enspace R \enspace a\)
     + *Symmetric* relations are such that
       * \(\forall\ a,b \in A, a\ R\ b \Rightarrow b\ R\ a\)
     + *Transitive* relations are such that
       * \(a\ R\ b \land \ \ b\ R\ c \Rightarrow \ a\ R\ c\)
	 * \(\forall\ a,b,c \in A\)

    If all of these are satisfied, the relation is said to be /an
    equivalence relation/.

***** Why?      
      Although this might seem needlessly pedantic, the algorithm we
      hope to use to find solutions to systems of polynomial
      equations, Buchberger's Algorithm, require us to decide on a
      way to order polynomials, for example in a quadratic equation
      it's pretty straight forward:

      \[
      f(x) = ax^2 + bx +c
      \]

      But for multiple variables it gets confusing, for example we could
      order the terms by degree, but if multiple terms are of the same
      degree then we could make sure that the left most variable has an
      exponent that is descending, or, we could try and make sure that
      the right most term is ascending:

      \begin{align}
       f\left(w,x,y,z\right)	&=wz+xy \\
			       &=xy+wz
      \end{align}

      This is already pretty confusing so having a firm definition of
      ordering is important.
   
**** Congruence   
***** Equivalence Classes
      The set of all elements of \(A\) that satisfy the relation for
      \(a\) is said to be the /equivalence class of \(a\) with respect to \(R\):

     \[\left\lbrack a \right\rbrack_{R} = \left\lbrack a \right\rbrack = \left\{ b \in A:b\ R\ a \right\}\]

     So returning to the example from \S [[#relation-example]], we would have:

     - \([1]_<=\emptyset \)
     - \([2]_<=\left\{1\right\}\)
     - \([3]_<=\left\{1, 2\right\}\)
***** Congruence Modulo \(n\)
      :PROPERTIES:
      :CUSTOM_ID: congruence-class
      :END:
      It is said that \(a\) and \(b\) are /congruent modulo \(n\)/ if
      \(n\mid \left(a-b\right)\) and it is written:

      \[
      a\equiv b \pmod{n}
      \]
      It is common to see \(\mod\) used as an operator:

      \[
      a \mod b = r
      \]

   
      The congruence class of \(a\) modulo \(n\) is expressed
      \(\left[a\right]_n\) and is the equivalence class of \(a\) whereby
      the relation is congruence in modulo \(n\):

   \[\left\lbrack a \right\rbrack_{n} = \{ b\mathbb{\in Z\ :}b \equiv a\ \left( \text{mod\ n} \right)\}\]   


****** Example
       Clock time is a congruence class, for example 11 O'clock + 3 hours
       = 2 PM:

       \[
       \left[11\right]_{12}+\left[3\right]_{12}=\left[2\right]_{12}
       \]

       Another example could be binary:


       \[
       \left[1\right]_{2}+\left[3\right]_{2}=\left[0\right]_{2}
       \]


       See also [[cite:roberthowlettUndergraduateCourseAbstract][\S 4c]]

****** Congruence generalised with Groups
       :PROPERTIES:
       :CUSTOM_ID: groups-modulo-equivalence-relation
       :END:
       If \(G\) is a group and \(H\) a subgroup, if we have:

       \[
       a^{-1}b \in H
       \]

       then it is said:

       \[
       a \equiv b \pmod{H}
       \]

       the use of "\(\equiv\)" is appropariate because the relationship
       is:

       - reflexive
       - symmetric
       - transitive

       and is hence an equiv class.

       consider for example:

       \[
       12 \mathbb{Z} \leqslant \mathbb{Z}
       \]

       so we have 5-17 \in 12 \mathbb{Z}

       So we write:

       \[
       5 \equiv 17 \pmod{12\mathbb{Z}}
       \]
    
       See [[cite:gregoryleeAbstractAlgebra2018][\S 3.7]].
    
****** Congruence Modulo an Ideal
       Congruence can be extended to an ideal on any ring structure,
       that's why we needed to generalise this structure, in order to use
       these theoryies.

       congruence modulo an ideal is

       If I is an ideal in a ring R

       \[
       a\equiv b\pmod{I}\iff a-b\in I
       \]

       The use of \equivis justified because this is an equivalence relation

       The equivalence class is the set of all elements that satisfy that
       relation for \(a\):


       \begin{align*}
             \forall a \in A,& \\
                             &\left[ a \right]_R = \left[ a \right] = \left\{b \in A : b r a \right\}
       .\end{align*}

       So in the context of congruence:

       \begin{align*}
             \foral a \in G &\\
                            & \left[ a \right] = \left\{b\in G : b\equiv a \pmod{H}\right\} 
       .\end{align*}

       if we wanted to find $b$ :

       \begin{align*}
             b &\equiv a \pmod{H}\\
             a^{-1}b &\in H \\
             a^{-1}b &= h, \quad \exists h \in H \\
             b &= ah
       .\end{align*}

       So we have:

       \begin{align*}
             \left[ a \right] = \left\{ah : h \in H\right\} 
       .\end{align*}

       This is known as the left coset
       [[cite:judsonAbstractAlgebraTheory2016][\S 6.1]]. The left cosets of $H$ 
       in $G$ partition G: [[cite:gregoryleeAbstractAlgebra2018][\S 3.3]]

	 1. Each $a\in G$ is in onlyone left coset, which is $aH$ 
	 2. $aH \cap bH = \emptyset$ or  $aH=bH$
    
       This can be used to show:

       \begin{align*}
             H \leq G \implies \left\lvert H \right\rvert \mid \left\lvert G \right\rvert
       .\end{align*}

       this is known as Lagranges Theorem. [[cite:gregoryleeAbstractAlgebra2018][\S 3.7]]


******* Normal Subgroups

       A normal subgroup is a subgroup $N \leq G$ :

       \begin{align*}
             aN= Na \quad \forall a \in G
       .\end{align*}

       This is not so strict as to require all elements be commutative (although
       commutative groups are of course normal)

****** TODO Congruence Classes for Polynomials
       If \(f\) and \(g\) are in an ideal \(I\), then [[cite:coxIdealsVarietiesAlgorithms1997][p. 240]]:

       \[
       f-g \in I \implies f \equiv g \pmod{I}
       \]
**** Groups
    A set \(G\) is a group, if there in a binary operation, \(\star\),
    defined on that set such that:

      1. The binary operation is closed on the set
	  \[a,b \in G \implies a\star b \in G\]
      2. The binary operation is associative

	  \[a,b,c \in G \implies a\star (b\star c) = (a\star b)\star c\]
      3. There is an element that doesn't do anything under the binary
	operation, this is known as an identity element, for example 1 is
	an identity element to the multiplication operation.

	  \begin{align*}
	  \exists e \in G:&\\
			  & a\star e = e \star a = a
	  \end{align*}
      4. Every element has an inverse

	  \begin{align*}
	  \forall a \in G,\enspace \exists a^{-1} \in G:	&\\
							   & a\star a^{-1} = e
	\end{align*}

	+ For operations that are additive in nature, it is common to use
	  the notation: \(-a\) [[cite:gregoryleeAbstractAlgebra2018][\S 3.3]]
      5. If the binary operation is also commutative, the group is said to be abelian:

       \begin{align*}
       \forall a,b \in G,& \nonumber \\
		       & a \star b = b \star a \iff G \text{ is abelian.} 
       \end{align*}
   
***** Example

      An example of a group is a set of all matrices of a given size under addition,
      this can be seen because:

	1. Adding matrices gives back matrices of the same size,
	2. Introducing brackets in addition doesn't change the result 
	3. A matrix with all 0's is an identity
	4. Any matrix \(\mathbf{A}\) has an inverse (namely \(-\mathbf{A}\))

      This example would also be an abelian group because addition is commutative.

      Note that if the operation was matrix multiplication, \(\cdot\)
      (denoted as ~%*%~ in */R/*
      cite:rcoreteamLanguageEnvironmentStatistical2020), only square
      matrices with a non-zero determinant
      (e.g. \(\left\lvert\mathbf{A}\right\rvert \neq 0\)) could be a
      group. This is because the matrix would need to be invertible. [fn:11]
   
***** But Why?
      The reason groups are interesting is because many natural
      structures can be described by a set and a binary operation,
      obvious examples are sets of numbers, vectors, matrices and
      equations, but more generally Group theory can be used to describe
      puzzles like /Rubik's Cube/ cite:joynerAdventuresGroupTheory2002,
      chemical structures cite:GroupTheoryIts2013 and has been used in
      the theory of
      quantum mechanics cite:tinkhamGroupTheoryQuantum2003. [fn:22]

**** Rings
     :PROPERTIES:
     :CUSTOM_ID: algebra-rings
     :END:
    Examples, equivalence class ring
    [[cite:judsonAbstractAlgebraTheory2016][Ch. 3]] see also \S 2.4 of
    nicodemii [[cite:nicodemiIntroductionAbstractAlgebra2007a][\S 2.4]]

    Rings are an abelian group under addition \(+\), with a second binary
    operation that corresponds to multiplication \(\times\), this
    operatuion must be closed, associative and distributive, but there is
    no need for an inverse or identity
    [[cite:gregoryleeAbstractAlgebra2018][\S 8.1]]. So a ring structure
    is a set \(\mathcal{R}\), with two closed binary operations, that
    satisfies the following axioms of a ring
    [[cite:nicodemiIntroductionAbstractAlgebra2007a][\S\S 2.4-2.6]]:

   1. Associativity of Addition

       \(\left( \forall a,b,c \in \mathcal{R} \right) \left( a+ b \right) +  c = a +  \left(  b +  c    \right)\)
    

   2. Commutativity of Addition

	 \(\left( \forall a,b \in \mathcal{R}  \right) a +  b = b +  a\)
      
   3. Additive Elements Exist

	 \(\left( \forall a \in \mathcal{R} \right) \wedge \left( \exists 0 \in \mathcal{R} \right) a +  0= 0 +  a =  a\)
      
   4. Additive Inverse Exists

	 \(\left( \forall a \in \mathcal{R} \right)\wedge \left( \exists b \in \mathcal{R} \right) a +  b =  b +  a = 0\)

       - This can be equivalently expressed:

       \(\left( \forall a \in \mathcal{R} \right)\wedge \left( \exists \left( - a\right)\in \mathcal{R} \right) a +  \left( - a \right) = \left( - a \right) +  a = 0\)

   5. Associativity of Multiplication

	 \(\left( \forall a,b,c, \in \mathcal{R} \right)\left( a \cdot  b \right)\cdot c = a \cdot  \left( b \cdot  c \right)\)

   6. Distributivity of Multiplication over Addition

     - \(\left( \forall a,b,c, \in \mathcal{R} \right) \left( a\cdot  \left( b+ c \right)=  \left( a \cdot   b  \right) +  \left( a \cdot   c  \right) \right)\), AND
     - \(\left( \forall a,b,c, \in \mathcal{R} \right)\left( a +  b \right)\cdot   c = \left( a \cdot   c  \right)+  \left( b \cdot   c \right)\)
***** Further Axioms

   Other conditions that correspond to different classes of rings are:

     7. Commutativity of Multiplication
	 - A ring that satisfies this property is called a *commutative ring*

           \(\left( \forall a,b \in \mathcal{R} \right) a \cdot  b = b \cdot  a\)

     8. Existence of a Multiplicative Identity Element (A ring with Unity)
       - A ring that satisfies this property is called a *ring with identity* or
       equivalently a *ring with unity* (the multiplicative identity, often
       denoted by \(1\), is called the *unity* of the ring.

           \(\left( \exists 1 \in \mathcal{R} \right) \left( \forall a \in \mathcal{R} \right) 1 \cdot  a = a \cdot  1 = a\)
***** Example
      An obvious example of a ring is the set of all integers
      \(\mathbb{Z}\) with the ordinary meaning of addition and
      multiplication. A more insightful example would be a congruence
      class, for example \(\mathbb{Z}_{12}\), this satisfies the axioms
      of a ring, but some values are zero divisors. If two elements of a
      ring multiply to give 0, those values are said to be zero divisors,
      for example 3 and 4 are zero divisors in \mathbb{Z}_{12}:

      \[
       \left[3\right]_{12}\times\left[4\right]_{12}=\left[0\right]_{12}
      \]

      An element that has an inverse is said to be a unit, for example:

      \[
     \left[2\right]_{9}\times\left[5\right]_{9}=\left[1\right]_{9}
     \]
     An element can't both be a unit and a zero divisor, because one
     multiplies to give 0 and the other to give 1, however, in many
     algebraic structures (e.g. \(\mathbb{Q}, \mathbb{R}\) or
     \(\mathbb{C} \)) every element has a multiplicative inverse, and
     this motivates the next algebraic structure. 

**** Integral Domains
     An integral domain is a commutative ring with identity that has no
     zero divisors.
***** Example
      The obvious example of an integral domain is \(\mathbb{Z}\), but any
      \(\mathbb{Z}_p\) where \(p\) is a prime number, will also be an integral domain.

      Another example is the set of all polynomials with real
      coefficients, this will be explored in greater detail below, but
      for the moment observe that this algebraic structure conforms to
      the axioms of a ring and has no zero divisors.

      It can be clearly seen that the set of polynomials has no zero
      divisors because:

      \begin{align}
      f \times g &= 0 \\
      &\implies f = 0, \lor g = 0 \ \\
      \end{align}

      in either case \(f\) or \(g\) is not a non-zero divisor.

      Note however that not every element of the polynomials has an
      inverse, for example the function \(f(x)=x\) would have an inverse
      \(f^{-1}(x)=\frac{1}{x}\), but this is not a polynomial.

      This leads to the final algebraic structure that will be considered
      here. [fn:4]
**** Fields
     :PROPERTIES:
     :CUSTOM_ID: algebra-fields
     :END:
     A field is a commutative ring with identity in which all non-zero
     elements are units.

     Because every element of a field is a unit, it
     implies that every element is not a zero-divisor, and so hence a
     field is:

     - a special case of an integral domain, which is in turn
     - a special case of a ring, which is in turn
     - a special case of a group.

**** Rings and Integral Domains
     It seems that the reason the theory of Groebner Bases is concerned
     with the ring of polynomials over a field is related to the
     irreducibility of the polynomial, see generally cite:EquivalenceDefinitionsIrreducible.

     Note also that the Ring of polynomials over an integral domain (a
     property satisfied also by a field) is more accurately an
     integral domain
     cite:sympydevelopmentteamBasicFunctionalityModule,RingPolynomialForms,
     not merely a ring.
***** Why aren't Polynomials fields
      A field is an integral domain, for which every element has an
      inverse, so consider some function, say $g(x)=x$, if the set of polynomials
      was a field, there would have to exist some $f(x)$ such that:

      \[
      x \cdot f(x) = 1
      \]

      however if we evaluate this at \(x=0\)

      \[
      0 \cdot f(0) = 1
      \]

      well... this clearly doesn't work, so it's clear that this \(f(x)\)
      doesn't exist and so the set of polynomials is not a field, see
      generally cite:billdubuqueAbstractAlgebraWhy

      One might wonder if there's a good reason why $f(x)=\frac{1}{x}$
      isn't considered a polynomial, notwithstanding the fact that it
      doesn't quite fix this example with 0:

	- All polynomials over the real numbers are continuous, that
	  would make this membership inconvenient.
	  - On the other hand there are discontinuities of arbitrary
            polynomials over certain fields, what's a good example of
            a such a field though?


	The easy and uninformative answer is that \(\frac{1}{x}\) does
	not have positive indices, outright violating the definition.
	   
*** Vector Spaces
    The ring of polynomials over a field \(K\):

    \[
    K\left[x_1, x_2, x_3, \ldots, x_n\right]
    \]

    is a \(n\)-vector space with a basis given by the set of all power products:

    \[
    \left\{x_1^{\beta_1}, x_2^{\beta_2}, x_3^{\beta_3}, \ldots x_n^{\beta_n} \right\}
    \]
**** Basis
     A basis is a set of vectors that
     [[cite:axlerLinearAlgebraDone2014][p. 39]] are:

       - Linearly independent
       - Spans an \(n\)-dimensional vector space??
***** Linear Independence
      a set of vectors are linearly independent if:

      \[
      a_{1}v_{1}+a_{2}v_{2}+a_{3}v_{3}\ldots=0 \iff a_{1}=a_{2}=a_{3}=\ldots=a_{m}
      \]
***** Span
      The span of a set of vectors, is the set of all possible linear
      combinations of those vectors.

      So for example:
      \begin{align}
      \mathbb{R}^2&=\mathrm{span}\left( \left\{\left(0,1\right), \left(1, 0\right)\right\}  \right)\\
		  &=\mathrm{span}\left( \left\{\left(0,2\right), \left(2, 0\right)\right\}  \right)\\
		  &=\mathrm{span}\left( \left\{\left(1,1\right), \left(1, -1\right)\right\}  \right)\\
      \end{align}

      To visualize this in \(\mathbb{R}^2\), imagine that by varying
      the scaling value of each vector, any point on \(\mathbb{R}^2\)
      can be reached.

**** Vectors
    A ring with unity is a vector space, however a vector space only
    needs to be closed under scalar multiplication. This means
    vector spaces are not necessarily rings unless the
    multiplication operation is closed, an example of a closed
    vector multiplication is element-wise multiplication, this is
    known as the hadamard product (think like mutliplying `numpy` arrays.)
** Monomial Orders
   :PROPERTIES:
   :CUSTOM_ID: monomial-orders
   :END:
    [[file:20210406222024-groebner_bases_of_a_system_of_equations.org][groebner bases of a system of equations]]
A partial order on a set is a relation $R$:

- $x R x$

  - reflexivity

- $x R y \land y R x \implies x = y$

  - Antisymmetry

- $x R y \land y R z \implies x R z$

  - Transitivity

So for example, the set of integers has $\leq$ as a relation such that
$n_1\in \mathbb{Z}:$

- $n\leq n$

- $n_1\leq n_2 \land n_2 \leq n_1 \implies n_1=n_2$

- $n_1\leq n_2 \land n_2 \leq n_3 \implies n_1\leq n_3$

A partially ordered set is one with a relation that is a partial order.

- partial order

  - a relation

- partially ordered set

  - a set

A total order is a partial order such that $\forall x,y$ either $x R y$
or $y R x$, the obvious example is $<$, consider for example
$\mathbb{C}$, this has a partial order if the the modulus is considered,
it's only a partial order because, e.g.
$\left\lvert i+i \right\rvert= \left\lvert i-i \right\rvert$. not all
sets will have a partial ordering, e.g. the somewhat contrived example
has no (at least obvious) partial order.

$$\left\{\square, \triangle, \sqrt{-1} x^{e^x} \right\} 
.$$

$k\left[ \mathbf{X} \right]$ is a polynomial ring in $n$ variables and
$\mathcal{M}_n$ is the set of amonomials in the variables
$x_1, x_2, x_3, \ldots x_n$.

A monomial order on $k\left[ \mathbf{X} \right]$ is a total order on
$\prec$ on $\mathcal{M}_n$:

1. $i \prec u, \quad \forall 1\in u\in \mathcal{M}_n$

2. $u, v\in \mathcal{M}_n \land u \prec v \implies uw \prec vw, \forall w \in \mathcal{M}_n$

**** Lexical monomial order
     :PROPERTIES:
     :CUSTOM_ID: lexical-monomial-order
     :END:
 Let:

 $$\begin{aligned}
     u &= x_1^{a_1} x_2^{a_2} x_3^{a_3} \ldots x_n^{a_n} \\
     v &= x_1^{b_1} x_2^{b_2} x_3^{b_3} \ldots x_n^{b_n}
 .\end{aligned}$$ The lexicographic order on $k\left( \mathbf{X} \right)$
 is given by the total order $<_{\mathrm{lex}}$ on $\mathcal{M}_n$ by
 setting:

 $$\begin{aligned}
     u <_{\mathrm{lex}} v
 .\end{aligned}$$

 if:

 1. $\sum^{n}_{i=1}\left[ a_i  \right] \leq \sum^{n}_{i=1}\left[ b_i  \right]$

 2. the leftmost non-zero term in the following vector is positive:

    - b_1-a_1, b_2-a_2, b_3-a_3 ...b_n-a_n

 Reverse lexicographic is:

 1. $\sum^{n}_{i=1}\left[ a_i  \right] \leq \sum^{n}_{i=1}\left[ b_i  \right]$

 2. the *rightmost* non-zero term in the following vector is *negative*:

    - b_1-a_1, b_2-a_2, b_3-a_3 ...b_n-a_n

 These should be combined into one statement $\uparrow$

 So for example consider: $$x_1x_4-x_2x_3
 .$$

 by lexicographic we have

 $$x_2x_3\prec x_1x_4
 .$$

 because the leftmost entry is positive in the vector described before:

 $$\left\langle 1, -1, -1, 1\right \rangle
 .$$

 by reverse lexicographic we have

 $$x_1x_4 \prec x_2x_3
 .$$

 because the *rightmost* entry is *negative* in the vector described
 before:

 $$\left\langle -1, 1, 1, -1\right \rangle
 .$$

 This may be discussed more in the org mode note.

 an interesting property that comes back in the buchberger algorithm and
 polynomial long division is:

 $$\mathrm{in}_{\prec}\left( f \cdot g \right) = \mathrm{in}_{\prec}\left( f \right) \mathrm{in}_{\prec}\left( g \right) 
 .$$

**** Colloquial 
***** Lexicographic
     The highest variable is so expensive that it makes the entire
     monomial expensive.
***** Reverse Lexicographic
      The lowest variable is so chap that it makes the entire monomial cheap.
** Dickson's Lemma
*** Divisors
    For /monomials/:

      - \(u= \prod^n_{i=1}\left[ x_i^a_i \right] \quad a \in \mathbb{Z^+} \)
      - \(v= \prod^n_{i=1}\left[ x_i^b_i \right] \quad b \in \mathbb{Z^+} \)
 
   $u$ is said to divide $v$ if $a_i \leq b_i \quad \forall i \in \left[ 1, n \right]$
 
**** Example

    Consider:

      - $u = x^2y^3z^5$
      - $v = x^1y^2z^3$

    In this case $v \mid u$ because:

    \begin{align*}
          1 &< 2 \\
          2 &< 3 \\
          3 &< 5 \\
          \ \\
          \frac{u}{v} &= \frac{x^2}{x^1} \cdot \frac{y^3}{y^2} \cdot  \frac{z^5}{z^3}
    .\end{align*}

*** Minimal Element

    let \(\mathcal{M}_n\) be the set of a all monomials in the variables
    $x_1, x_2, x_3, \ldots x_n$ and \(M \subset \mathcal{M}_n\) be a
    nonempty subset thereof.

    The following condition describes a minimal element \(u\in M\):

    \[
    \left(v \in M \land v \mid u\right) \implies v = u
    \]

    In other words, \(u\) is a minimal element if the only way that \(v
    \mid u \) is if \(v = u\).

  
**** Example

     Consider $\mathcal{M}_2$:
   
     \begin{alignat}{3}
       \mathcal{M}_2 &= \{&x  y, &x  y^2, &x  y^3, \ldots         \\
                     &    &      &x^2y,   &x^2y^2, x^2y^3, \ldots \\
                     &    &      &x^3y,   &x^3y^2, x^3y^3, \ldots \\
                     &    &      & \vdots &                       \\
		       \}
     \end{alignat}

     and let's have the subset \(M = \left\{ x^2y, x^2y^2, x^2y^3 \ldots
     \right\}\), the minimum elements are:

     \[
           \left\{x^2y\right\}
     \]


     clearly \(\left\lvert M \right\rvert = \infty\), however this number of
     mi \centernot\implies um elements will always be finite, this is known as
     *Dickson's Lemma*.
     
*** Dickson's Lemma

    #+BEGIN_QUOTE
    /Dickson's Lemma is the main result needed to prove the termination
    of Buchberger's algorithm for computing Groebner basis of polynomial
    ideals/ cite:martin-mateosFormalProofDickson2003.
    #+END_QUOTE

    Let
      - \(\mathcal{M}_n\) be the set of all monomials in variables \(x_1, x_2, x_3 \ldots x_n\).
      - \(M\) be a nonempty subset of \(\mathcal{M}_n\)
  
    #+BEGIN_QUOTE
    /The set of minimal elements of a nonempty subset \(M \subset
    \mathcal{M}_n\) is at most finite./
    #+END_QUOTE

    This intuitively makes sense, I can't have an infinite number of
    minimums, otherwise they wouldn't be minimums, the proof is very
    difficult though.

**** In one Variable

     By definition, a monomial is raised to the power of a non-zero
     integer, in a single variable monomial the smallest index will
     correspond to the minimal element (by the definition of the minimal
     element) and hence the existence of a minimum element in
     \mathbb{Z^+} implies the existence of a minimum element in
     \(M\subset \mathcal{M}_n\).

   
**** In Two Variables

     Assume that there is an infinite number of minimal elements:

     \begin{align}
           u_1 &= x^{a_1}y^{b_1} \\
           u_2 &= x^{a_2}y^{b_2} \\
           u_3 &= x^{a_3}y^{b_3} \\
           u_4 &= x^{a_4}y^{b_4} \\
           u_5 &= x^{a_5}y^{b_5} \\
           \ldots \nonumber
     \end{align}
     Let's order the values by the first exponential such that \(a_1 \leq a_2 \leq a_3 \ldots\).

     If \(a_i=a_{i+1}\), then either:

       - \(u_1 = u_{i+1}\)
	 - We can't have this because set's do not contain repeated elements.
       - \(y^{b_i} \neq y^{b_{i+1}}\)
	 - But this would mean that either \(u_i\) or \(u_{i+1\}\) is
           not a minimal element, so this can't occur either.
       
   
     This means that each \(a_i\) must be different and so:

     \begin{align}
     a_i < a_2 < a_3 \ldots
     \end{align}

     If \(u_i | u_{i+1}\) one of them is not a minimal element and so we
     must have \(b_i > b_{i+1}\), hence \(b_i > b_2 > b_3 \ldots\).

     This means that \(b_i\) represents an upper bound for the number of
     different minimal elements, hence the number of minimal elements
     must be finite.

   
  
**** In \(n\) variables                                                         :induction:
     If the number of minimal elements is finite for \(M_n \subset
     \mathcal{M}_n\) we would expect \(M_{n+1}\) to be finite as well,
     adding an extra variable should not make the number of minimal
     elements infinite because the integers in the index will still
     behave as an upper bound.

     I need to formalise this as per [[cite:hibiGrobnerBasesStatistics2014][\S 1.1.2]].
* Footnotes

[fn:3] This also lines up with =sympy='s =LT()= function, beware not to
confuse the initial with the leading term, different algorithms or
ways to calculate an \(S\)-polynomial seem to use either and it
doesn't matter, I'm not sure why yet, but I am sure that there is a
difference between the initial monomial and the leading term.

[fn:1] It would also be sufficient to show that the \(I\) is closed
under both addition and subtraction [[cite:judsonAbstractAlgebraTheory2016][\S 16.1]]

[fn:2] In the absence of better materials a lot of time was wasted
(yes, wasted, not spent) on complex algebraic concepts when all I
needed was an algorithm to experiment with, an algorithm that the
complex texts would not provide.



[fn:4] There are other algebraic structures that could be interesting,
for example polynomials can also be considered as vectors, see
e.g. cite:larsonElementaryLinearAlgebra1991a, as a matter of fact all
vector spaces are rings if multiplication is defined element-wise by
the /Hadamard product/ (\(\odot\)), this could be an interesting
relationship to investigate further.

[fn:33] Relevant because we need to decide on an ordering relation in
order to use Buchberger's algorithm, which is needed to find a
Groebner Basis. 
[fn:22] See generally this cite:19328 /Stack Exchange Discussion/. 

[fn:11] although the element-wise product, \(\odot\), would not present this issue.
